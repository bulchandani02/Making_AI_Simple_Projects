# -*- coding: utf-8 -*-
"""context_management.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BPEnxFRoPxTZKoQb43GMY_cQORbWm0Lh

## Step 1: Install Dependencies
"""

!pip install langchain langchain-community langchain-text-splitters -q
!pip install chromadb sentence-transformers -q
!pip install nltk -q
import nltk
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)
print("All packages installed!")

"""## Step 2: Load a Long Document (TechCorp Technical Architecture)"""

# A realistic long-form technical document
LONG_DOCUMENT = """
# TechCorp Technical Architecture Document

## 1. System Overview

TechCorp's platform is the backbone of our operations, processing 2.3 million API requests daily across 12 global regions. The platform has evolved over 5 years from a monolithic application to a modern microservices architecture.

Our primary tech stack consists of Python with FastAPI for backend services, PostgreSQL for persistent storage, Redis for caching, and Kubernetes on AWS for orchestration. This combination was chosen after extensive benchmarking against alternatives including Go with Gin, and Node.js with Express.

The migration to microservices began in 2021 and completed in 2023. During this transition, we maintained 99.9% uptime through careful blue-green deployments and feature flags. The key lesson learned was that microservices aren't always the answer - we actually consolidated some services back into larger units when the overhead wasn't justified.

## 2. Service Architecture

### 2.1 Core Services

Our architecture consists of 23 microservices organized into 4 domains: Authentication, Data Processing, Machine Learning, and Search.

**AuthService** handles all authentication and authorization. It implements OAuth 2.0 with OIDC for single sign-on, supporting both internal employees and external API consumers. The service maintains a 99.99% uptime SLA, achieved through multi-region deployment with automatic failover. Token validation uses RS256 signatures with rotating keys every 24 hours.

**DataPipeline** is built on Apache Kafka and processes approximately 50,000 events per second during peak hours. Events flow through three stages: ingestion, transformation, and storage. The transformation layer uses Apache Flink for real-time aggregations. One critical design decision was implementing exactly-once semantics, which added complexity but eliminated data inconsistencies that plagued our earlier at-least-once implementation.

**MLInference** serves our machine learning models through TensorFlow Serving. Average latency is 45ms at p50 and 120ms at p99. We use model versioning to enable A/B testing and gradual rollouts. The service automatically scales based on request queue depth, typically running 8-24 replicas depending on load.

**SearchService** powers all search functionality using an Elasticsearch cluster with 500 million documents indexed. We use a custom ranking algorithm that combines BM25 with learned embeddings. Search latency targets are 100ms at p50 and 300ms at p99.

### 2.2 Database Architecture

Our data layer follows a polyglot persistence strategy, using different databases for different use cases.

**PostgreSQL 15** serves as our primary relational database for transactional data. We run a primary instance with read replicas in each region. Connection pooling through PgBouncer keeps connection counts manageable. The largest table contains 2.3 billion rows and is partitioned by date.

**Redis Cluster** provides caching with a 6-node setup achieving 99.95% cache hit rate. We use Redis for session storage, rate limiting, and frequently accessed configuration. Cache invalidation follows a write-through pattern for critical data and TTL-based expiration for others.

**Snowflake** handles our analytics workload with over 50TB of historical data. ETL pipelines run hourly, with critical metrics updated every 15 minutes. The data warehouse supports our business intelligence dashboards and ad-hoc analysis.

### 2.3 Inter-Service Communication

Services communicate through a combination of synchronous REST APIs and asynchronous message queues.

REST APIs use gRPC internally for performance-critical paths, with REST/JSON exposed externally for ease of integration. All APIs are versioned using URL path versioning (e.g., /v1/, /v2/). We maintain backward compatibility for at least 12 months after deprecation announcements.

Message queues handle asynchronous workflows through Kafka topics. Each domain owns its topics, and cross-domain communication follows an event-driven architecture. We implemented the outbox pattern to ensure reliable message publishing alongside database transactions.

## 3. Deployment & CI/CD

### 3.1 Deployment Strategy

All production deployments use blue-green deployment for zero-downtime releases. The process works as follows:

1. Deploy new version to "green" environment
2. Run automated smoke tests against green
3. Gradually shift traffic: 5% â†’ 25% â†’ 50% â†’ 100%
4. Monitor error rates and latency at each stage
5. Keep blue environment ready for instant rollback

Canary releases extend this for risky changes, holding at 5% traffic for up to 24 hours while monitoring dashboards. Rollback time is under 2 minutes, achieved through pre-warmed instances and connection draining.

### 3.2 CI/CD Pipeline

Our pipeline runs on GitHub Actions with the following stages:

**Build Stage**: Compiles code, runs unit tests (target: 80% coverage), and builds Docker images. Build time averages 4 minutes for most services.

**Test Stage**: Runs integration tests against ephemeral environments. Each PR gets its own namespace in our test Kubernetes cluster. Tests must pass with 100% success rate.

**Security Stage**: Runs SAST (Semgrep), DAST (OWASP ZAP), and container scanning (Trivy). Any critical or high vulnerabilities block deployment.

**Deploy Stage**: Pushes images to ECR, updates ArgoCD application manifests, and triggers deployment. ArgoCD handles the actual Kubernetes rollout with health checks.

Average time from merge to production: 8 minutes for standard deployments, 15 minutes when security scans flag items for review.

## 4. Monitoring & Observability

### 4.1 Monitoring Stack

We follow the three pillars of observability: metrics, logs, and traces.

**Metrics**: Prometheus scrapes all services every 15 seconds. We use the RED method (Rate, Errors, Duration) for service-level metrics and USE method (Utilization, Saturation, Errors) for infrastructure. Grafana dashboards provide visualization with alerts configured for SLO breaches.

**Logs**: All services emit structured JSON logs to Fluent Bit, which forwards to Elasticsearch. We retain 30 days of logs in hot storage and 1 year in cold storage. Log correlation uses trace IDs to link related entries across services.

**Traces**: Jaeger provides distributed tracing with 100% sampling for errors and 1% sampling for successful requests. Traces help identify latency bottlenecks and debug cross-service issues.

### 4.2 Alerting and Incident Response

PagerDuty handles alert routing based on service ownership and severity. Response time SLAs:

- **P1 (Critical)**: Data breach, complete outage. Response within 15 minutes, all hands on deck.
- **P2 (High)**: Partial outage, significant degradation. Response within 1 hour, on-call engineer.
- **P3 (Medium)**: Minor issues, non-critical bugs. Response within 4 hours, next business day OK.
- **P4 (Low)**: Cosmetic issues, improvements. Response within 24 hours, scheduled maintenance.

Post-incident reviews happen within 48 hours for P1/P2 incidents. We maintain a blameless culture focused on systemic improvements.

### 4.3 SLAs and SLOs

We define SLOs at the service level with error budgets:

| Service | Availability SLO | Latency SLO (p99) | Error Budget (monthly) |
|---------|------------------|-------------------|------------------------|
| API Gateway | 99.95% | 200ms | 21.6 minutes |
| AuthService | 99.99% | 100ms | 4.3 minutes |
| DataPipeline | 99.5% | N/A (async) | 3.6 hours |
| MLInference | 99.9% | 150ms | 43.2 minutes |
| SearchService | 99.9% | 300ms | 43.2 minutes |

When error budget is exhausted, we freeze feature releases and focus on reliability improvements.

## 5. Security Architecture

### 5.1 Network Security

All traffic flows through our defense-in-depth architecture:

**Edge Layer**: Cloudflare provides DDoS protection and WAF. We block approximately 2 million malicious requests daily. Rate limiting is enforced at 1000 requests per minute per API key.

**Load Balancer Layer**: AWS ALB terminates TLS and routes to appropriate services. We use TLS 1.3 exclusively, with older versions disabled.

**Service Mesh**: Istio provides mTLS between all services. No plaintext traffic exists within the cluster. Service-to-service authentication uses SPIFFE identities.

### 5.2 Data Security

Data protection follows classification levels:

- **Public**: No restrictions. Marketing content, documentation.
- **Internal**: Requires authentication. Employee information, internal wikis.
- **Confidential**: Requires authorization + audit logging. Customer data, financial reports.
- **Restricted**: Requires MFA + manager approval + full audit trail. Encryption keys, PII, credentials.

All data at rest uses AES-256 encryption. Customer data is encrypted with customer-specific keys where required for compliance.

## 6. Disaster Recovery

### 6.1 Backup Strategy

We maintain multiple backup layers:

**Database Backups**: PostgreSQL continuous archiving to S3 with point-in-time recovery capability. Full backups weekly, incremental daily. Retention: 30 days hot, 1 year cold.

**Configuration Backups**: All Kubernetes manifests and configurations stored in Git. Infrastructure as Code through Terraform with state in S3.

**Data Replication**: Synchronous replication to standby region for critical services. Asynchronous replication for analytics data.

### 6.2 Recovery Objectives

| Tier | Services | RPO | RTO |
|------|----------|-----|-----|
| Tier 1 | Auth, API Gateway | 0 (sync replication) | 5 minutes |
| Tier 2 | Core business services | 15 minutes | 30 minutes |
| Tier 3 | Analytics, reporting | 1 hour | 4 hours |

We conduct disaster recovery drills quarterly, simulating region failures and measuring actual recovery times against objectives.
"""

print(f" Loaded document: {len(LONG_DOCUMENT):,} characters")
print(f" Approximately {len(LONG_DOCUMENT.split()):,} words")

"""## Step 3: The Problem with Fixed-Size Chunking"""

from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter

# Naive fixed-size chunking
naive_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=0,
    separator=""  # Split anywhere
)

naive_chunks = naive_splitter.split_text(LONG_DOCUMENT)

print("="*60)
print(" PROBLEM: Fixed-Size Chunking")
print("="*60)
print(f"\nCreated {len(naive_chunks)} chunks of ~500 chars each")
print("\nLook at chunk 5 - see how it cuts mid-sentence:\n")
print("-"*40)
print(naive_chunks[5])
print("-"*40)

print("\n\n The Issues:")
print("1. Cuts sentences in the middle")
print("2. Separates related information")
print("3. Loses section context (which part of doc is this from?)")
print("4. Search might find half a thought")

"""## Step 4: Recursive Character Splitting (Better)"""

# Recursive splitting - respects natural boundaries
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n## ", "\n### ", "\n\n", "\n", ". ", " "]
)

recursive_chunks = recursive_splitter.split_text(LONG_DOCUMENT)

print("="*60)
print(" BETTER: Recursive Character Splitting")
print("="*60)
print(f"\nCreated {len(recursive_chunks)} chunks")
print("\nLook at a chunk - respects sentence boundaries:\n")
print("-"*40)
print(recursive_chunks[5])
print("-"*40)

print("\n\n Improvements:")
print("1. Tries to split at '## ' (sections) first")
print("2. Falls back to '### ' (subsections)")
print("3. Then paragraphs, then sentences")
print("4. Only splits mid-sentence as last resort")

"""## Step 5: Semantic Chunking (Best for Meaning)"""

"""
Semantic Chunking splits based on MEANING, not characters.

The idea:
1. Split into sentences
2. Create embeddings for each sentence
3. Compare adjacent sentences
4. Split where similarity drops (topic change)
"""

from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re

def semantic_chunking(text, similarity_threshold=0.5, min_chunk_size=100):
    """
    Split text where meaning changes significantly.

    Args:
        text: Document text
        similarity_threshold: Split when similarity drops below this
        min_chunk_size: Minimum characters per chunk

    Returns:
        List of semantically coherent chunks
    """
    # Split into sentences
    sentences = re.split(r'(?<=[.!?])\s+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    if len(sentences) < 2:
        return [text]

    # Get embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(sentences)

    # Find breakpoints where similarity drops
    breakpoints = []
    for i in range(1, len(embeddings)):
        sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]
        if sim < similarity_threshold:
            breakpoints.append(i)

    # Create chunks between breakpoints
    chunks = []
    start = 0
    for bp in breakpoints:
        chunk = ' '.join(sentences[start:bp])
        if len(chunk) >= min_chunk_size:
            chunks.append(chunk)
            start = bp

    # Add remaining
    if start < len(sentences):
        chunk = ' '.join(sentences[start:])
        if len(chunk) >= min_chunk_size:
            chunks.append(chunk)
        elif chunks:
            chunks[-1] += ' ' + chunk

    return chunks

# Demo with a portion of the document
sample_section = LONG_DOCUMENT.split("## 3.")[0].split("## 2.")[-1]

print("="*60)
print(" SEMANTIC CHUNKING")
print("="*60)
print("\nSplitting based on meaning changes...")

semantic_chunks = semantic_chunking(sample_section, similarity_threshold=0.45)

print(f"\nCreated {len(semantic_chunks)} semantic chunks")
print("\nFirst semantic chunk:\n")
print("-"*40)
print(semantic_chunks[0][:500] + "..." if len(semantic_chunks[0]) > 500 else semantic_chunks[0])
print("-"*40)

"""## Step 6: Parent-Child Retrieval (The Game Changer)"""

"""
Parent-Child Retrieval Pattern:

Problem:
- Small chunks = precise search but missing context
- Large chunks = good context but imprecise search

Solution:
- Create SMALL chunks for searching (child)
- Link each to its LARGER parent chunk
- Search on children, return parents

Example:
Parent (1000 chars): Entire "Deployment Strategy" section
Children (200 chars each): Individual paragraphs within

Search finds: "blue-green deployment" in child chunk
Return: Entire deployment section for full context
"""

class ParentChildChunker:
    """
    Creates hierarchical chunks - small for search, large for context.
    """

    def __init__(self, parent_chunk_size=1500, child_chunk_size=300, child_overlap=50):
        self.parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=parent_chunk_size,
            chunk_overlap=100,
            separators=["\n## ", "\n### ", "\n\n", "\n", ". "]
        )
        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=child_chunk_size,
            chunk_overlap=child_overlap,
            separators=["\n\n", "\n", ". ", " "]
        )

    def create_hierarchy(self, text):
        """
        Create parent and child chunks with links.

        Returns:
            parents: List of large chunks
            children: List of small chunks with parent_id
        """
        parents = self.parent_splitter.split_text(text)

        all_children = []
        for parent_id, parent in enumerate(parents):
            children = self.child_splitter.split_text(parent)
            for child in children:
                all_children.append({
                    "text": child,
                    "parent_id": parent_id,
                    "parent_preview": parent[:100] + "..."
                })

        return parents, all_children

# Create hierarchy
chunker = ParentChildChunker()
parents, children = chunker.create_hierarchy(LONG_DOCUMENT)

print("="*60)
print("PARENT-CHILD RETRIEVAL")
print("="*60)
print(f"\nParents: {len(parents)} large chunks (~1500 chars each)")
print(f"Children: {len(children)} small chunks (~300 chars each)")
print(f"Average children per parent: {len(children)/len(parents):.1f}")

print("\n\nExample Parent (truncated):")
print("-"*40)
print(parents[3][:400] + "...")
print("-"*40)

print("\n\nIts Children:")
parent_3_children = [c for c in children if c['parent_id'] == 3]
for i, child in enumerate(parent_3_children[:3]):
    print(f"\n  Child {i+1}: {child['text'][:100]}...")

"""## Step 7: Parent-Child Search in Action"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class ParentChildRAG:
    """
    Search on children, return parents for context.
    """

    def __init__(self, parents, children):
        self.parents = parents
        self.children = children
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

        # Embed children only (for search)
        print("Embedding child chunks...")
        child_texts = [c['text'] for c in children]
        self.child_embeddings = self.model.encode(child_texts, show_progress_bar=True)

    def search(self, query, top_k=3):
        """
        Search children, return unique parents.
        """
        query_emb = self.model.encode([query])

        # Find top matching children
        scores = cosine_similarity(query_emb, self.child_embeddings)[0]
        top_indices = np.argsort(scores)[::-1][:top_k * 2]  # Get extra to find unique parents

        # Get unique parents
        seen_parents = set()
        results = []

        for idx in top_indices:
            child = self.children[idx]
            parent_id = child['parent_id']

            if parent_id not in seen_parents:
                seen_parents.add(parent_id)
                results.append({
                    "parent": self.parents[parent_id],
                    "matched_child": child['text'],
                    "score": float(scores[idx])
                })

            if len(results) >= top_k:
                break

        return results

# Build the search system
pc_rag = ParentChildRAG(parents, children)

# Test it
print("\n" + "="*60)
print("PARENT-CHILD SEARCH DEMO")
print("="*60)

test_query = "How does blue-green deployment work?"
results = pc_rag.search(test_query, top_k=2)

print(f"\nQuery: '{test_query}'")
print("\n" + "-"*40)

for i, r in enumerate(results):
    print(f"\n Result {i+1} (score: {r['score']:.3f})")
    print(f"\n Matched on (child):")
    print(f"   '{r['matched_child'][:150]}...'")
    print(f"\n Returned (parent - full context):")
    print(f"   '{r['parent'][:300]}...'")
    print("-"*40)

"""## Step 8: Sliding Window with Smart Overlap"""

"""
Sliding Window ensures no information is lost at chunk boundaries.

Without overlap:
  Chunk 1: "...deployment uses blue-green"
  Chunk 2: "strategy for zero downtime..."

  Query "blue-green strategy" might miss both!

With overlap:
  Chunk 1: "...deployment uses blue-green strategy for zero..."
  Chunk 2: "...blue-green strategy for zero downtime releases..."

  Query finds it in both chunks.
"""

def sliding_window_chunks(text, window_size=500, stride=350):
    """
    Create overlapping chunks with sliding window.

    Args:
        text: Document text
        window_size: Size of each chunk
        stride: How far to move window (smaller = more overlap)

    Returns:
        List of overlapping chunks
    """
    chunks = []
    start = 0

    while start < len(text):
        end = start + window_size
        chunk = text[start:end]

        # Try to end at sentence boundary
        if end < len(text):
            last_period = chunk.rfind('. ')
            if last_period > window_size * 0.7:  # At least 70% of chunk
                chunk = chunk[:last_period + 1]

        chunks.append({
            "text": chunk,
            "start": start,
            "end": start + len(chunk)
        })

        start += stride

    return chunks

# Demo
window_chunks = sliding_window_chunks(LONG_DOCUMENT, window_size=600, stride=400)

print("="*60)
print("SLIDING WINDOW CHUNKING")
print("="*60)
print(f"\nWindow size: 600 chars")
print(f"Stride: 400 chars (200 char overlap)")
print(f"Created {len(window_chunks)} overlapping chunks")

# Show overlap
print("\n\nOverlap demonstration:")
print("-"*40)
print(f"Chunk 3 ends with:")
print(f"  '...{window_chunks[2]['text'][-100:]}'")
print(f"\nChunk 4 starts with:")
print(f"  '{window_chunks[3]['text'][:100]}...'")
print("-"*40)
print("\n See how they share text? Queries won't miss boundary information.")

"""## Step 9: Document Hierarchy for Structured Content"""

"""
For documents with clear structure (like technical docs),
preserve the hierarchy: Document â†’ Section â†’ Subsection â†’ Paragraph

This enables:
1. Filtering by section
2. Understanding document structure
3. Better context in answers
"""

import re

def parse_document_hierarchy(text):
    """
    Parse markdown-style document into hierarchy.

    Returns nested structure preserving document organization.
    """
    hierarchy = {
        "title": "",
        "sections": []
    }

    lines = text.split('\n')
    current_section = None
    current_subsection = None
    current_content = []

    for line in lines:
        # Document title
        if line.startswith('# ') and not hierarchy['title']:
            hierarchy['title'] = line[2:].strip()

        # Section header
        elif line.startswith('## '):
            # Save previous section
            if current_section:
                if current_subsection:
                    current_subsection['content'] = '\n'.join(current_content)
                    current_section['subsections'].append(current_subsection)
                elif current_content:
                    current_section['content'] = '\n'.join(current_content)
                hierarchy['sections'].append(current_section)

            current_section = {
                'title': line[3:].strip(),
                'subsections': [],
                'content': ''
            }
            current_subsection = None
            current_content = []

        # Subsection header
        elif line.startswith('### '):
            if current_subsection:
                current_subsection['content'] = '\n'.join(current_content)
                current_section['subsections'].append(current_subsection)

            current_subsection = {
                'title': line[4:].strip(),
                'content': ''
            }
            current_content = []

        # Regular content
        elif line.strip():
            current_content.append(line)

    # Save last section
    if current_section:
        if current_subsection:
            current_subsection['content'] = '\n'.join(current_content)
            current_section['subsections'].append(current_subsection)
        elif current_content:
            current_section['content'] = '\n'.join(current_content)
        hierarchy['sections'].append(current_section)

    return hierarchy

# Parse document
doc_hierarchy = parse_document_hierarchy(LONG_DOCUMENT)

print("="*60)
print("DOCUMENT HIERARCHY")
print("="*60)
print(f"\nDocument: {doc_hierarchy['title']}")
print(f"\nSections ({len(doc_hierarchy['sections'])}):")

for section in doc_hierarchy['sections']:
    print(f"\n   {section['title']}")
    for subsection in section['subsections']:
        print(f"       {subsection['title']}")

"""## Step 10: Hierarchy-Aware Chunking"""

def create_hierarchical_chunks(hierarchy, chunk_size=500):
    """
    Create chunks that preserve hierarchical metadata.
    """
    chunks = []
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=50
    )

    for section in hierarchy['sections']:
        section_title = section['title']

        # Chunk section content
        if section.get('content'):
            section_chunks = splitter.split_text(section['content'])
            for chunk in section_chunks:
                chunks.append({
                    'text': chunk,
                    'section': section_title,
                    'subsection': None,
                    'path': f"{hierarchy['title']} > {section_title}"
                })

        # Chunk subsection content
        for subsection in section['subsections']:
            if subsection.get('content'):
                sub_chunks = splitter.split_text(subsection['content'])
                for chunk in sub_chunks:
                    chunks.append({
                        'text': chunk,
                        'section': section_title,
                        'subsection': subsection['title'],
                        'path': f"{hierarchy['title']} > {section_title} > {subsection['title']}"
                    })

    return chunks

# Create hierarchical chunks
hier_chunks = create_hierarchical_chunks(doc_hierarchy)

print("="*60)
print("HIERARCHICAL CHUNKS")
print("="*60)
print(f"\nCreated {len(hier_chunks)} chunks with metadata")

print("\n\nExample chunk with hierarchy info:")
print("-"*40)
example = hier_chunks[15]
print(f" Path: {example['path']}")
print(f" Section: {example['section']}")
print(f" Subsection: {example['subsection']}")
print(f"\n Content: {example['text'][:200]}...")
print("-"*40)

# Show distribution
print("\n\nChunks per section:")
from collections import Counter
section_counts = Counter(c['section'] for c in hier_chunks)
for section, count in section_counts.most_common():
    print(f"  {section}: {count} chunks")

"""## Step 11: Filtered Search with Hierarchy"""

class HierarchicalRAG:
    """
    RAG that can filter by document structure.
    """

    def __init__(self, chunks):
        self.chunks = chunks
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

        print("Embedding chunks...")
        texts = [c['text'] for c in chunks]
        self.embeddings = self.model.encode(texts, show_progress_bar=True)

    def search(self, query, top_k=3, section_filter=None):
        """
        Search with optional section filtering.
        """
        query_emb = self.model.encode([query])
        scores = cosine_similarity(query_emb, self.embeddings)[0]

        # Apply section filter
        if section_filter:
            for i, chunk in enumerate(self.chunks):
                if section_filter.lower() not in chunk['section'].lower():
                    scores[i] = -1  # Exclude from results

        top_idx = np.argsort(scores)[::-1][:top_k]

        return [{
            'text': self.chunks[i]['text'],
            'path': self.chunks[i]['path'],
            'section': self.chunks[i]['section'],
            'score': float(scores[i])
        } for i in top_idx]

# Build hierarchical RAG
hier_rag = HierarchicalRAG(hier_chunks)

print("\n" + "="*60)
print("HIERARCHICAL SEARCH DEMO")
print("="*60)

# Search without filter
query = "What are the response time requirements?"
print(f"\n Query: '{query}'")

print("\n\n Without section filter:")
results = hier_rag.search(query, top_k=2)
for r in results:
    print(f"\n   {r['path']}")
    print(f"     Score: {r['score']:.3f}")
    print(f"     '{r['text'][:100]}...'")

print("\n\n With filter: 'Monitoring' section only")
results = hier_rag.search(query, top_k=2, section_filter="Monitoring")
for r in results:
    print(f"\n   {r['path']}")
    print(f"     Score: {r['score']:.3f}")
    print(f"     '{r['text'][:100]}...'")

"""## Step 12: Comparison - Which Strategy When?"""

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    CHUNKING STRATEGIES - DECISION GUIDE                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  ðŸ“ FIXED SIZE                                                               â•‘
â•‘     When: Quick prototyping, uniform content (logs, transcripts)             â•‘
â•‘     Avoid: Structured docs, technical content                                â•‘
â•‘     Chunk size: 500-1000 chars                                               â•‘
â•‘                                                                              â•‘
â•‘  ðŸ”„ RECURSIVE CHARACTER                                                      â•‘
â•‘     When: General purpose, mixed content types                               â•‘
â•‘     Good for: Most RAG applications (solid default)                          â•‘
â•‘     Chunk size: 500-1500 chars, 10-20% overlap                               â•‘
â•‘                                                                              â•‘
â•‘  ðŸ§  SEMANTIC                                                                 â•‘
â•‘     When: Need meaning-based splits, topic detection                         â•‘
â•‘     Good for: News articles, research papers, varied topics                  â•‘
â•‘     Trade-off: Slower (requires embedding each sentence)                     â•‘
â•‘                                                                              â•‘
â•‘  ðŸ‘¨â€ðŸ‘©â€ðŸ‘§ PARENT-CHILD                                                            â•‘
â•‘     When: Need precise search BUT full context in answers                    â•‘
â•‘     Best for: Technical docs, manuals, detailed procedures                   â•‘
â•‘     Children: 200-400 chars | Parents: 1000-2000 chars                       â•‘
â•‘                                                                              â•‘
â•‘  ðŸ“Š SLIDING WINDOW                                                           â•‘
â•‘     When: Can't afford to miss boundary information                          â•‘
â•‘     Good for: Legal documents, compliance, contracts                         â•‘
â•‘     Overlap: 20-30% of chunk size                                            â•‘
â•‘                                                                              â•‘
â•‘  ðŸ—ï¸ HIERARCHICAL                                                             â•‘
â•‘     When: Documents have clear structure (markdown, HTML, manuals)           â•‘
â•‘     Enables: Section filtering, breadcrumb context                           â•‘
â•‘     Best for: Documentation, policies, technical specs                       â•‘
â•‘                                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                           PRODUCTION RECOMMENDATION                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘   Start with: Recursive Character (solid baseline)                           â•‘
â•‘   Add Parent-Child: When answers lack context                                â•‘
â•‘   Add Hierarchical: When docs have clear structure                           â•‘
â•‘   Consider Semantic: When topics vary wildly within docs                     â•‘
â•‘                                                                              â•‘
â•‘   ðŸŽ¯ Most production systems: Recursive + Parent-Child = 90% of value        â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

"""## Step 13: Production-Ready Context Manager"""

class ContextManager:
    """
    Production-ready context management for RAG.

    Combines multiple chunking strategies with smart defaults.
    """

    def __init__(self, strategy="parent_child"):
        """
        Args:
            strategy: 'recursive', 'parent_child', 'hierarchical', 'semantic'
        """
        self.strategy = strategy
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.chunks = []
        self.parents = []
        self.embeddings = None

    def process_document(self, text, doc_id=None):
        """
        Process document with selected strategy.
        """
        if self.strategy == "recursive":
            self._process_recursive(text, doc_id)
        elif self.strategy == "parent_child":
            self._process_parent_child(text, doc_id)
        elif self.strategy == "hierarchical":
            self._process_hierarchical(text, doc_id)
        else:
            self._process_recursive(text, doc_id)

        # Create embeddings
        texts = [c['text'] for c in self.chunks]
        self.embeddings = self.model.encode(texts, show_progress_bar=True)

        return len(self.chunks)

    def _process_recursive(self, text, doc_id):
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100
        )
        chunks = splitter.split_text(text)
        for i, chunk in enumerate(chunks):
            self.chunks.append({
                'text': chunk,
                'doc_id': doc_id,
                'chunk_id': i,
                'strategy': 'recursive'
            })

    def _process_parent_child(self, text, doc_id):
        parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)
        child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)

        parents = parent_splitter.split_text(text)
        self.parents = parents

        for parent_id, parent in enumerate(parents):
            children = child_splitter.split_text(parent)
            for child in children:
                self.chunks.append({
                    'text': child,
                    'parent_id': parent_id,
                    'parent_text': parent,
                    'doc_id': doc_id,
                    'strategy': 'parent_child'
                })

    def _process_hierarchical(self, text, doc_id):
        hierarchy = parse_document_hierarchy(text)
        chunks = create_hierarchical_chunks(hierarchy)
        for chunk in chunks:
            chunk['doc_id'] = doc_id
            chunk['strategy'] = 'hierarchical'
            self.chunks.append(chunk)

    def search(self, query, top_k=5, return_parent=True):
        """
        Search chunks with optional parent return.
        """
        query_emb = self.model.encode([query])
        scores = cosine_similarity(query_emb, self.embeddings)[0]
        top_idx = np.argsort(scores)[::-1][:top_k]

        results = []
        seen_parents = set()

        for idx in top_idx:
            chunk = self.chunks[idx]

            # For parent-child, return parent and deduplicate
            if self.strategy == "parent_child" and return_parent:
                parent_id = chunk.get('parent_id')
                if parent_id in seen_parents:
                    continue
                seen_parents.add(parent_id)

                results.append({
                    'matched_chunk': chunk['text'],
                    'context': chunk.get('parent_text', chunk['text']),
                    'score': float(scores[idx])
                })
            else:
                results.append({
                    'text': chunk['text'],
                    'metadata': {k: v for k, v in chunk.items() if k != 'text'},
                    'score': float(scores[idx])
                })

        return results[:top_k]


# Demo
print("="*60)
print("PRODUCTION CONTEXT MANAGER")
print("="*60)

cm = ContextManager(strategy="parent_child")
num_chunks = cm.process_document(LONG_DOCUMENT, doc_id="tech_arch_v1")

print(f"\n Processed document: {num_chunks} chunks")
print(f"   Strategy: {cm.strategy}")
print(f"   Parents: {len(cm.parents)}")

# Test search
query = "What happens during a deployment?"
results = cm.search(query, top_k=2)

print(f"\n\n Query: '{query}'")
for i, r in enumerate(results):
    print(f"\n Result {i+1} (score: {r['score']:.3f})")
    print(f"   Matched: '{r['matched_chunk'][:80]}...'")
    print(f"   Context: '{r['context'][:150]}...'")

"""## Step 14: Export Production Module"""

production_code = '''
"""
Context Management Module - Making AI Simple Day 15
Multiple chunking strategies for RAG systems.

Usage:
    from context_manager import ContextManager

    cm = ContextManager(strategy="parent_child")
    cm.process_document(text, doc_id="my_doc")
    results = cm.search("query", top_k=5)

Strategies:
    - recursive: General purpose (default)
    - parent_child: Precise search, full context return
    - hierarchical: Structure-aware for markdown/HTML docs
"""

from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re


class ContextManager:
    def __init__(self, strategy="parent_child"):
        self.strategy = strategy
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.chunks = []
        self.parents = []
        self.embeddings = None

    def process_document(self, text, doc_id=None):
        if self.strategy == "parent_child":
            self._process_parent_child(text, doc_id)
        elif self.strategy == "hierarchical":
            self._process_hierarchical(text, doc_id)
        else:
            self._process_recursive(text, doc_id)

        texts = [c['text'] for c in self.chunks]
        self.embeddings = self.model.encode(texts)
        return len(self.chunks)

    def _process_recursive(self, text, doc_id):
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        for i, chunk in enumerate(splitter.split_text(text)):
            self.chunks.append({'text': chunk, 'doc_id': doc_id, 'chunk_id': i})

    def _process_parent_child(self, text, doc_id):
        parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)
        child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)

        self.parents = parent_splitter.split_text(text)
        for pid, parent in enumerate(self.parents):
            for child in child_splitter.split_text(parent):
                self.chunks.append({
                    'text': child, 'parent_id': pid,
                    'parent_text': parent, 'doc_id': doc_id
                })

    def search(self, query, top_k=5, return_parent=True):
        query_emb = self.model.encode([query])
        scores = cosine_similarity(query_emb, self.embeddings)[0]
        top_idx = np.argsort(scores)[::-1]

        results, seen = [], set()
        for idx in top_idx:
            chunk = self.chunks[idx]
            if self.strategy == "parent_child" and return_parent:
                pid = chunk.get('parent_id')
                if pid in seen: continue
                seen.add(pid)
                results.append({
                    'matched': chunk['text'],
                    'context': chunk.get('parent_text', chunk['text']),
                    'score': float(scores[idx])
                })
            else:
                results.append({'text': chunk['text'], 'score': float(scores[idx])})
            if len(results) >= top_k: break
        return results


if __name__ == "__main__":
    test_doc = """# Test Doc
    ## Section 1
    This is section one with some content about deployment strategies.
    ## Section 2
    This section covers monitoring and alerting best practices.
    """
    cm = ContextManager(strategy="parent_child")
    cm.process_document(test_doc)
    print(cm.search("deployment"))
'''

with open("context_manager.py", "w") as f:
    f.write(production_code)

print("Saved: context_manager.py")