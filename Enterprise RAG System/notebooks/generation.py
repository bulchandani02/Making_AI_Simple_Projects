# -*- coding: utf-8 -*-
"""generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rOT0dFMS3v6gXlC-IARxcAUg-sMSJEdR

## Step 1: Install Dependencies
"""

!pip install langchain langchain-community langchain-text-splitters -q
!pip install chromadb sentence-transformers -q
!pip install google-generativeai -q
print("All packages installed!")

"""## Step 2: Setup LLM"""

import google.generativeai as genai
from google.colab import userdata

# Configure Gemini (free tier)
try:
    genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))
    llm = genai.GenerativeModel('gemini-1.5-flash')
    LLM_AVAILABLE = True
    print("Gemini API configured!")
except:
    LLM_AVAILABLE = False
    print("No API key found. Add GOOGLE_API_KEY to Colab secrets.")
    print("Some demos will use mock responses.")

def generate(prompt, temperature=0.3):
    """Generate response from LLM."""
    if not LLM_AVAILABLE:
        return "[Mock response - add GOOGLE_API_KEY for real outputs]"

    response = llm.generate_content(
        prompt,
        generation_config=genai.types.GenerationConfig(temperature=temperature)
    )
    return response.text

"""## Step 3: Load Retrieved Context (Simulating Days 12-15)"""

# Simulating what our retrieval system would return
# In production, this comes from your RAG pipeline

RETRIEVED_CHUNKS = [
    {
        "id": 1,
        "text": """Blue-green deployment allows zero-downtime releases. The process works as follows:
1. Deploy new version to "green" environment
2. Run automated smoke tests against green
3. Gradually shift traffic: 5% - 25% - 50% - 100%
4. Monitor error rates and latency at each stage
5. Keep blue environment ready for instant rollback
Rollback time is under 2 minutes, achieved through pre-warmed instances and connection draining.""",
        "source": "technical_architecture.md",
        "section": "Deployment Strategy"
    },
    {
        "id": 2,
        "text": """Canary releases extend blue-green for risky changes, holding at 5% traffic for up to 24 hours while monitoring dashboards. This approach was adopted after a major incident in 2022 where a full rollout caused 45 minutes of downtime.""",
        "source": "technical_architecture.md",
        "section": "Deployment Strategy"
    },
    {
        "id": 3,
        "text": """Our CI/CD pipeline runs on GitHub Actions with the following stages:
Build Stage: Compiles code, runs unit tests (target: 80% coverage), builds Docker images. Build time averages 4 minutes.
Test Stage: Runs integration tests against ephemeral environments. Each PR gets its own namespace.
Average time from merge to production: 8 minutes for standard deployments.""",
        "source": "technical_architecture.md",
        "section": "CI/CD Pipeline"
    }
]

print("Loaded 3 retrieved chunks (simulating retrieval from Days 12-15)")
for chunk in RETRIEVED_CHUNKS:
    print(f"   [{chunk['id']}] {chunk['source']} > {chunk['section']}")

"""## Step 4: The Problem - Naive Generation"""

"""
Most tutorials show this pattern:

prompt = f"Context: {context}\n\nQuestion: {question}\n\nAnswer:"

This fails because:
1. No instruction on HOW to use the context
2. No guidance on what to do if context doesn't have answer
3. No citation requirement
4. LLM defaults to its training data, not your context
"""

def format_context(chunks):
    """Simple context formatting."""
    return "\n\n".join([f"[{c['id']}] {c['text']}" for c in chunks])

# Naive prompt
naive_prompt = f"""Context:
{format_context(RETRIEVED_CHUNKS)}

Question: What is the deployment rollback time and why is it so fast?

Answer:"""

print("="*60)
print("NAIVE PROMPT - THE PROBLEM")
print("="*60)
print(naive_prompt[:500] + "...")

if LLM_AVAILABLE:
    naive_response = generate(naive_prompt)
    print("\n\nNaive Response:")
    print("-"*40)
    print(naive_response)
    print("-"*40)
    print("\nProblems with this approach:")
    print("   - May not cite sources")
    print("   - Might add information not in context")
    print("   - No clear grounding to retrieved chunks")

"""## Step 5: Grounded Generation Prompt"""

"""
Grounded Generation = Force LLM to ONLY use provided context.

Key elements:
1. Explicit instruction to use ONLY the context
2. Tell it to say "I don't know" if answer isn't there
3. Require citations
4. Specify output format
"""

GROUNDED_PROMPT_TEMPLATE = """You are a helpful assistant answering questions based on company documentation.

INSTRUCTIONS:
- Answer ONLY using the information provided in the CONTEXT below
- If the context doesn't contain the answer, say "I don't have information about that in the available documents"
- Cite your sources using [1], [2], etc. matching the chunk IDs
- Be concise and direct
- Do not make up information or use external knowledge

CONTEXT:
{context}

QUESTION: {question}

ANSWER:"""

def grounded_generate(question, chunks):
    """Generate grounded response with citations."""
    context = format_context(chunks)
    prompt = GROUNDED_PROMPT_TEMPLATE.format(context=context, question=question)
    return generate(prompt)

print("="*60)
print("GROUNDED GENERATION - THE SOLUTION")
print("="*60)

question = "What is the deployment rollback time and why is it so fast?"
if LLM_AVAILABLE:
    grounded_response = grounded_generate(question, RETRIEVED_CHUNKS)
    print(f"\nQuestion: {question}")
    print("\nGrounded Response:")
    print("-"*40)
    print(grounded_response)
    print("-"*40)

"""## Step 6: Testing Hallucination Prevention"""

"""
The real test: Ask something NOT in the context.
A good RAG system should admit it doesn't know.
"""

print("="*60)
print("HALLUCINATION TEST")
print("="*60)

# Question where answer is NOT in retrieved chunks
trick_question = "What programming language is the backend written in?"

print(f"\nQuestion: {trick_question}")
print("   (Note: This info is NOT in our retrieved chunks)")

if LLM_AVAILABLE:
    # Naive approach - might hallucinate
    naive_response = generate(f"Context:\n{format_context(RETRIEVED_CHUNKS)}\n\nQuestion: {trick_question}\n\nAnswer:")
    print("\nNaive Response (may hallucinate):")
    print(f"   {naive_response[:200]}...")

    # Grounded approach - should decline
    grounded_response = grounded_generate(trick_question, RETRIEVED_CHUNKS)
    print("\nGrounded Response (should decline):")
    print(f"   {grounded_response}")

"""## Step 7: Proper Citation Formatting"""

"""
Citations build trust. Users need to verify answers.

Two approaches:
1. Inline citations: "The rollback time is 2 minutes [1]."
2. End citations: Answer first, then "Sources: [1] technical_architecture.md"

We'll implement both.
"""

CITATION_PROMPT_TEMPLATE = """You are a helpful assistant answering questions based on company documentation.

INSTRUCTIONS:
- Answer ONLY using the information in the CONTEXT below
- Use inline citations like [1], [2] immediately after each fact
- Every claim must have a citation
- If context doesn't have the answer, say "I don't have information about that"
- At the end, list all sources you cited

CONTEXT:
{context}

QUESTION: {question}

Provide your answer with inline citations, then list sources:"""

def generate_with_citations(question, chunks):
    """Generate response with proper citations."""
    # Format context with clear IDs
    context_parts = []
    for chunk in chunks:
        context_parts.append(f"[{chunk['id']}] Source: {chunk['source']} | Section: {chunk['section']}\n{chunk['text']}")

    context = "\n\n---\n\n".join(context_parts)
    prompt = CITATION_PROMPT_TEMPLATE.format(context=context, question=question)
    return generate(prompt)

print("="*60)
print("CITATION FORMATTING")
print("="*60)

question = "Explain the deployment process and how long it takes."
if LLM_AVAILABLE:
    cited_response = generate_with_citations(question, RETRIEVED_CHUNKS)
    print(f"\nQuestion: {question}")
    print("\nResponse with Citations:")
    print("-"*40)
    print(cited_response)
    print("-"*40)

"""## Step 8: Answer Synthesis from Multiple Chunks"""

"""
Real questions often need information from MULTIPLE chunks.
The LLM needs to synthesize a coherent answer.

Challenge: Don't just concatenate. Create a flowing response.
"""

SYNTHESIS_PROMPT_TEMPLATE = """You are a helpful assistant answering questions based on company documentation.

INSTRUCTIONS:
- Synthesize information from multiple sources into ONE coherent answer
- Don't just list facts - create a narrative that flows naturally
- Use citations [1], [2], etc. for each piece of information
- If sources have conflicting information, mention both perspectives
- Start with the most important information first

CONTEXT:
{context}

QUESTION: {question}

Synthesized Answer:"""

def synthesize_answer(question, chunks):
    """Synthesize coherent answer from multiple chunks."""
    context_parts = []
    for chunk in chunks:
        context_parts.append(f"[{chunk['id']}] {chunk['source']} > {chunk['section']}:\n{chunk['text']}")

    context = "\n\n".join(context_parts)
    prompt = SYNTHESIS_PROMPT_TEMPLATE.format(context=context, question=question)
    return generate(prompt)

print("="*60)
print("ANSWER SYNTHESIS")
print("="*60)

# Question that requires multiple chunks
synthesis_question = "Give me a complete overview of how code gets deployed to production, from merge to live."

if LLM_AVAILABLE:
    synthesized = synthesize_answer(synthesis_question, RETRIEVED_CHUNKS)
    print(f"\nQuestion: {synthesis_question}")
    print("\nSynthesized Answer:")
    print("-"*40)
    print(synthesized)
    print("-"*40)

"""## Step 9: Advanced Hallucination Guards"""

"""
Beyond prompt engineering, we can add programmatic checks:

1. Citation verification - Did the LLM actually cite provided sources?
2. Claim extraction - Extract claims and verify against chunks
3. Confidence scoring - How much of the answer is grounded?
"""

import re

def verify_citations(response, chunks):
    """Verify that citations in response match provided chunks."""
    # Extract citation IDs from response
    cited_ids = set(re.findall(r'\[(\d+)\]', response))
    cited_ids = {int(id) for id in cited_ids}

    # Get available chunk IDs
    available_ids = {chunk['id'] for chunk in chunks}

    # Check for invalid citations
    invalid_citations = cited_ids - available_ids
    valid_citations = cited_ids & available_ids

    return {
        "cited_sources": list(valid_citations),
        "invalid_citations": list(invalid_citations),
        "uncited_sources": list(available_ids - cited_ids),
        "citation_coverage": len(valid_citations) / len(available_ids) if available_ids else 0
    }

def calculate_grounding_score(response, chunks):
    """
    Estimate how grounded the response is.
    Simple approach: Check keyword overlap with chunks.
    """
    # Combine all chunk text
    chunk_text = " ".join([c['text'].lower() for c in chunks])
    chunk_words = set(chunk_text.split())

    # Get response words
    response_words = set(response.lower().split())

    # Filter to meaningful words (remove common words)
    stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
                  'to', 'of', 'and', 'in', 'that', 'it', 'for', 'on', 'with'}
    chunk_words = chunk_words - stop_words
    response_words = response_words - stop_words

    # Calculate overlap
    overlap = response_words & chunk_words
    grounding_ratio = len(overlap) / len(response_words) if response_words else 0

    return {
        "grounding_score": round(grounding_ratio, 2),
        "grounded_terms": len(overlap),
        "total_terms": len(response_words)
    }

print("="*60)
print("HALLUCINATION VERIFICATION")
print("="*60)

if LLM_AVAILABLE:
    # Generate a response
    test_response = generate_with_citations(
        "How does deployment work?",
        RETRIEVED_CHUNKS
    )

    print("\nGenerated Response:")
    print(test_response[:300] + "...")

    # Verify citations
    citation_check = verify_citations(test_response, RETRIEVED_CHUNKS)
    print("\n\nCitation Verification:")
    print(f"   Valid citations: {citation_check['cited_sources']}")
    print(f"   Invalid citations: {citation_check['invalid_citations']}")
    print(f"   Coverage: {citation_check['citation_coverage']:.0%}")

    # Check grounding
    grounding = calculate_grounding_score(test_response, RETRIEVED_CHUNKS)
    print(f"\nGrounding Score: {grounding['grounding_score']:.0%}")
    print(f"   ({grounding['grounded_terms']}/{grounding['total_terms']} terms from context)")

"""## Step 10: Response Formatting Options"""

"""
Different use cases need different formats:
- Chat: Conversational, friendly
- Documentation: Formal, structured
- API: JSON with metadata
"""

FORMAT_TEMPLATES = {
    "conversational": """You are a friendly assistant helping employees find information.

Based on these documents:
{context}

Question: {question}

Respond conversationally but accurately. Use [1], [2] citations. If you're not sure, say so.""",

    "formal": """Based on the provided documentation, answer the following query.

DOCUMENTATION:
{context}

QUERY: {question}

Provide a formal, structured response with proper citations [1], [2].
Use bullet points for multiple items. Be precise and professional.""",

    "json": """You are an API that returns structured responses.

CONTEXT:
{context}

QUESTION: {question}

Return a JSON response with this structure:
{{
    "answer": "your answer here",
    "citations": [1, 2],
    "confidence": "high/medium/low",
    "sources": ["source1.md", "source2.md"]
}}

Only return valid JSON, nothing else."""
}

def generate_formatted(question, chunks, format_type="conversational"):
    """Generate response in specified format."""
    context = format_context(chunks)
    template = FORMAT_TEMPLATES.get(format_type, FORMAT_TEMPLATES["conversational"])
    prompt = template.format(context=context, question=question)
    return generate(prompt)

print("="*60)
print("RESPONSE FORMATS")
print("="*60)

test_q = "What happens if a deployment fails?"

if LLM_AVAILABLE:
    for fmt in ["conversational", "formal", "json"]:
        print(f"\n\n{'='*20} {fmt.upper()} {'='*20}")
        response = generate_formatted(test_q, RETRIEVED_CHUNKS, fmt)
        print(response[:400] + "..." if len(response) > 400 else response)

"""## Step 11: Production RAG Generator Class"""

class RAGGenerator:
    """
    Production-ready answer generator for RAG systems.

    Features:
    - Grounded generation
    - Citation formatting
    - Hallucination checks
    - Multiple output formats
    """

    def __init__(self, llm_generate_fn):
        """
        Args:
            llm_generate_fn: Function that takes prompt, returns response
        """
        self.generate_fn = llm_generate_fn

    def generate(
        self,
        question: str,
        chunks: list,
        format_type: str = "conversational",
        verify_citations: bool = True
    ) -> dict:
        """
        Generate grounded answer with citations.

        Args:
            question: User question
            chunks: Retrieved chunks with 'id', 'text', 'source' keys
            format_type: 'conversational', 'formal', or 'json'
            verify_citations: Whether to verify citation accuracy

        Returns:
            Dict with answer, citations, grounding score
        """
        # Build prompt
        prompt = self._build_prompt(question, chunks, format_type)

        # Generate response
        response = self.generate_fn(prompt)

        # Build result
        result = {
            "question": question,
            "answer": response,
            "format": format_type,
            "num_chunks": len(chunks)
        }

        # Verify if requested
        if verify_citations:
            result["citation_check"] = self._verify_citations(response, chunks)
            result["grounding_score"] = self._calculate_grounding(response, chunks)

        return result

    def _build_prompt(self, question, chunks, format_type):
        """Build appropriate prompt based on format."""
        context_parts = []
        for chunk in chunks:
            source = chunk.get('source', 'Unknown')
            section = chunk.get('section', '')
            context_parts.append(
                f"[{chunk['id']}] {source}" +
                (f" > {section}" if section else "") +
                f"\n{chunk['text']}"
            )
        context = "\n\n---\n\n".join(context_parts)

        base_instruction = """Answer ONLY using the provided context.
If the answer isn't in the context, say "I don't have information about that."
Use [1], [2] citations for each fact."""

        if format_type == "formal":
            style = "Be formal and structured. Use bullet points where appropriate."
        elif format_type == "json":
            style = """Return JSON: {"answer": "...", "citations": [1,2], "confidence": "high/medium/low"}"""
        else:
            style = "Be conversational and helpful."

        return f"""{base_instruction}
{style}

CONTEXT:
{context}

QUESTION: {question}

ANSWER:"""

    def _verify_citations(self, response, chunks):
        """Check citation validity."""
        cited = set(re.findall(r'\[(\d+)\]', response))
        cited = {int(i) for i in cited}
        available = {c['id'] for c in chunks}

        return {
            "valid": list(cited & available),
            "invalid": list(cited - available),
            "coverage": len(cited & available) / len(available) if available else 0
        }

    def _calculate_grounding(self, response, chunks):
        """Calculate grounding score."""
        chunk_text = " ".join([c['text'].lower() for c in chunks])
        chunk_words = set(chunk_text.split())
        response_words = set(response.lower().split())

        stop = {'the','a','an','is','are','was','were','be','been','to','of','and','in','that','it','for','on','with','as','by','this','from'}
        chunk_words -= stop
        response_words -= stop

        overlap = response_words & chunk_words
        return round(len(overlap) / len(response_words), 2) if response_words else 0


# Demo
print("="*60)
print("PRODUCTION RAG GENERATOR")
print("="*60)

if LLM_AVAILABLE:
    generator = RAGGenerator(generate)

    result = generator.generate(
        question="How long does deployment take and what are the stages?",
        chunks=RETRIEVED_CHUNKS,
        format_type="conversational",
        verify_citations=True
    )

    print(f"\nQuestion: {result['question']}")
    print(f"\nAnswer:\n{result['answer']}")
    print(f"\nMetrics:")
    print(f"   Citation coverage: {result['citation_check']['coverage']:.0%}")
    print(f"   Grounding score: {result['grounding_score']:.0%}")
    print(f"   Valid citations: {result['citation_check']['valid']}")

"""## Step 12: Comparison - Prompt Patterns"""

print("""
========================================================================
                ANSWER GENERATION PATTERNS - COMPARISON
========================================================================

NAIVE PATTERN (Don't use this)
    "Context: {context}. Question: {question}. Answer:"
    Problems: Hallucinations, no citations, ignores context

GROUNDED PATTERN (Recommended)
    Key elements:
    - "Answer ONLY using the context"
    - "Say 'I don't know' if not in context"
    - "Cite sources using [1], [2]"
    Result: Reliable, verifiable answers

SYNTHESIS PATTERN (For multi-chunk answers)
    Key elements:
    - "Synthesize into ONE coherent answer"
    - "Don't just list - create narrative"
    - "Start with most important info"
    Result: Natural, flowing responses

VERIFIED PATTERN (Production systems)
    Add programmatic checks:
    - Citation verification (do IDs match?)
    - Grounding score (keywords from context?)
    - Confidence estimation
    Result: Measurable quality

------------------------------------------------------------------------
                          QUICK REFERENCE
------------------------------------------------------------------------

Prevent hallucinations:
    - "ONLY use provided context"
    - "If not in context, say I don't know"

Get citations:
    - "Cite using [1], [2] after each fact"
    - Number chunks clearly in context

Better synthesis:
    - "Create coherent narrative, don't list"
    - "Start with most important info"

Verify quality:
    - Check cited IDs exist
    - Calculate grounding score

========================================================================
""")

"""## Step 13: Export Production Module"""

production_code = '''
"""
RAG Answer Generator - Making AI Simple Day 16
Grounded generation with citations and hallucination prevention.

Usage:
    from answer_generator import RAGGenerator

    generator = RAGGenerator(your_llm_function)
    result = generator.generate(
        question="your question",
        chunks=[{"id": 1, "text": "...", "source": "..."}],
        format_type="conversational"
    )
    print(result["answer"])
    print(result["grounding_score"])

Author: Jyotsna
Series: Making AI Simple (Day 16/56)
"""

import re
from typing import List, Dict, Callable, Any, Optional


class RAGGenerator:
    """
    Production-ready answer generator with grounding and citations.
    """

    def __init__(self, llm_fn: Callable[[str], str]):
        """
        Args:
            llm_fn: Function that takes prompt string, returns response string
        """
        self.llm = llm_fn

    def generate(
        self,
        question: str,
        chunks: List[Dict[str, Any]],
        format_type: str = "conversational",
        verify: bool = True
    ) -> Dict[str, Any]:
        """
        Generate grounded answer with citations.

        Args:
            question: User question
            chunks: List of {"id": int, "text": str, "source": str, "section": str (optional)}
            format_type: "conversational", "formal", or "json"
            verify: Whether to verify citations and grounding

        Returns:
            {"answer": str, "citation_check": dict, "grounding_score": float}
        """
        prompt = self._build_prompt(question, chunks, format_type)
        response = self.llm(prompt)

        result = {
            "question": question,
            "answer": response,
            "format": format_type,
            "num_chunks": len(chunks)
        }

        if verify:
            result["citation_check"] = self._verify_citations(response, chunks)
            result["grounding_score"] = self._grounding_score(response, chunks)

        return result

    def _build_prompt(self, question: str, chunks: list, format_type: str) -> str:
        # Format context
        context_parts = []
        for c in chunks:
            header = f"[{c['id']}] {c.get('source', 'Unknown')}"
            if c.get('section'):
                header += f" > {c['section']}"
            context_parts.append(f"{header}\\n{c['text']}")

        context = "\\n\\n---\\n\\n".join(context_parts)

        # Base instruction
        instruction = """Answer ONLY using the provided context.
If the answer isn't in the context, say "I don't have information about that."
Cite sources using [1], [2] after each fact."""

        # Format-specific style
        styles = {
            "conversational": "Be helpful and conversational.",
            "formal": "Be formal and structured. Use bullet points where appropriate.",
            "json": 'Return JSON: {"answer": "...", "citations": [1,2], "confidence": "high/medium/low"}'
        }
        style = styles.get(format_type, styles["conversational"])

        return f"{instruction}\\n{style}\\n\\nCONTEXT:\\n{context}\\n\\nQUESTION: {question}\\n\\nANSWER:"

    def _verify_citations(self, response: str, chunks: list) -> dict:
        cited = {int(i) for i in re.findall(r'\\[(\\d+)\\]', response)}
        available = {c['id'] for c in chunks}
        valid = cited & available

        return {
            "valid": list(valid),
            "invalid": list(cited - available),
            "missing": list(available - cited),
            "coverage": len(valid) / len(available) if available else 0
        }

    def _grounding_score(self, response: str, chunks: list) -> float:
        chunk_text = " ".join([c['text'].lower() for c in chunks])
        chunk_words = set(chunk_text.split())
        response_words = set(response.lower().split())

        stop = {'the','a','an','is','are','was','were','be','been','to','of',
                'and','in','that','it','for','on','with','as','by','this','from',
                'can','will','would','should','could','may','might'}
        chunk_words -= stop
        response_words -= stop

        if not response_words:
            return 0.0

        overlap = response_words & chunk_words
        return round(len(overlap) / len(response_words), 2)


# Simple test
if __name__ == "__main__":
    # Mock LLM for testing
    def mock_llm(prompt):
        return "The deployment takes 8 minutes [3] and uses blue-green strategy [1]."

    gen = RAGGenerator(mock_llm)
    result = gen.generate(
        "How long does deployment take?",
        [
            {"id": 1, "text": "Blue-green deployment for zero downtime", "source": "doc.md"},
            {"id": 3, "text": "Average deployment time is 8 minutes", "source": "doc.md"}
        ]
    )
    print(f"Answer: {result['answer']}")
    print(f"Grounding: {result['grounding_score']}")
    print(f"Citations: {result['citation_check']}")
'''

with open("answer_generator.py", "w") as f:
    f.write(production_code)

print("Saved: answer_generator.py")