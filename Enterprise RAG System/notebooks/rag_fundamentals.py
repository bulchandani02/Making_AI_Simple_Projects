# -*- coding: utf-8 -*-
"""RAG Fundamentals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xp93Bm3IS2KT1YWSwqJ9z-SVbHBZcE3p

#Step 1: Install Dependencies
"""

!pip install langchain langchain-community langchain-text-splitters chromadb sentence-transformers tiktoken -q
print("All packages installed!")

"""#Step 2: Understanding RAG - The Concept

##### LLMs sometimes make up answers they don't know. RAG lets them look up facts first. Finds the right info from your documents. Uses that to give correct answers.​

#### Better lookup = better answers.

#Step 4: Chunking Strategies - Breaking Documents Into Pieces
"""

from langchain_text_splitters import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
    TokenTextSplitter
)

# Sample document
sample_document = """
# RAG Architecture Guide

## Introduction
Retrieval-Augmented Generation (RAG) is a technique that enhances Large Language Models
by providing them with relevant context from external knowledge sources. This approach
significantly reduces hallucinations and enables LLMs to answer questions about
private or recent data they weren't trained on.

## Core Components

### 1. Document Loader
The first step is loading your documents. RAG systems can process various formats:
- PDF files (research papers, manuals)
- Word documents (reports, proposals)
- Web pages (documentation, articles)
- Databases (structured data)

### 2. Text Splitter (Chunking)
Documents must be split into smaller chunks because:
- LLMs have context length limits
- Smaller chunks enable more precise retrieval
- Embeddings work better on focused content

Common chunking strategies:
- Fixed-size chunks (simple but may break sentences)
- Recursive splitting (respects document structure)
- Semantic chunking (groups related content)

### 3. Embedding Model
Embeddings convert text into numerical vectors that capture semantic meaning.
Similar concepts have similar vectors, enabling semantic search.

Popular embedding models:
- OpenAI text-embedding-3-small (fast, good quality)
- Cohere embed-v3 (multilingual)
- sentence-transformers (free, open-source)

### 4. Vector Database
Stores embeddings for fast similarity search. Options include:
- ChromaDB (local, easy to start)
- Pinecone (managed, scalable)
- Weaviate (open-source, feature-rich)
- Azure AI Search (enterprise, hybrid search)

### 5. Retriever
Given a query, finds the most relevant chunks from the vector database.
Key parameters:
- k: number of chunks to retrieve
- score threshold: minimum similarity score
- search type: similarity, MMR (diversity), hybrid

### 6. Generator (LLM)
Takes the query + retrieved context and generates an answer.
The prompt typically looks like:
"Given the following context, answer the question.
Context: [retrieved chunks]
Question: [user query]
Answer:"

## Best Practices

1. Chunk size matters: 500-1000 tokens is often optimal
2. Add overlap: 10-20% overlap prevents losing context at boundaries
3. Include metadata: source, page number, date for citations
4. Use hybrid search: combine keyword (BM25) + semantic (vector) search
5. Re-rank results: use cross-encoders to improve precision
6. Evaluate regularly: measure retrieval quality, not just generation

## Common Pitfalls

- Chunks too large → irrelevant content dilutes the answer
- Chunks too small → loses context, fragmented information
- Wrong embedding model → poor semantic matching
- No evaluation → no way to know if improvements help
"""

# Let's compare different chunking strategies
print("=" * 60)
print("CHUNKING STRATEGY COMPARISON")
print("=" * 60)

# Strategy 1: Fixed-size character splitting
char_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separator="\n"
)
char_chunks = char_splitter.split_text(sample_document)

print(f"\n Character Splitter (500 chars, 50 overlap)")
print(f"   Number of chunks: {len(char_chunks)}")
print(f"   First chunk preview: {char_chunks[0][:100]}...")

# Strategy 2: Recursive splitting (respects structure)
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n## ", "\n### ", "\n\n", "\n", " ", ""]
)
recursive_chunks = recursive_splitter.split_text(sample_document)

print(f"\n Recursive Splitter (respects headers)")
print(f"   Number of chunks: {len(recursive_chunks)}")
print(f"   First chunk preview: {recursive_chunks[0][:100]}...")

# Strategy 3: Token-based splitting
token_splitter = TokenTextSplitter(
    chunk_size=100,  # tokens, not characters
    chunk_overlap=10
)
token_chunks = token_splitter.split_text(sample_document)

print(f"\n Token Splitter (100 tokens)")
print(f"   Number of chunks: {len(token_chunks)}")
print(f"   First chunk preview: {token_chunks[0][:100]}...")

print("\n" + "=" * 60)
print(" KEY INSIGHT: Recursive splitting preserves document structure!")
print("   It tries to split at headers first, then paragraphs, then sentences.")
print("=" * 60)

"""# Step 4: Embeddings - Converting Text to Vectors"""

from sentence_transformers import SentenceTransformer
import numpy as np

# Load an open-source embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
print("Model loaded!")

# Let's see embeddings in action
sample_texts = [
    "RAG reduces hallucinations in LLMs",
    "Retrieval-Augmented Generation improves AI accuracy",
    "The weather is nice today",
    "Vector databases store embeddings efficiently"
]

# Generate embeddings
embeddings = embedding_model.encode(sample_texts)

print(f"\n Embedding Statistics:")
print(f"   Shape: {embeddings.shape}")
print(f"   Each text → {embeddings.shape[1]}-dimensional vector")

# Calculate similarity between texts
from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(embeddings)

print(f"\n Similarity Matrix:")
print("   (1.0 = identical meaning, 0.0 = completely different)")
print()

for i, text1 in enumerate(sample_texts):
    for j, text2 in enumerate(sample_texts):
        if i < j:
            sim = similarity_matrix[i][j]
            short1 = text1[:40] + "..." if len(text1) > 40 else text1
            short2 = text2[:40] + "..." if len(text2) > 40 else text2
            print(f"   '{short1}'\n   ↔ '{short2}'\n   Similarity: {sim:.3f}\n")

print(" Notice: Semantically similar texts have HIGH similarity scores!")
print(" This is how RAG finds relevant documents - by meaning, not keywords.")

"""# Step 5: Building the Vector Database"""

import chromadb
from chromadb.utils import embedding_functions

# Create ChromaDB client (in-memory for demo)
chroma_client = chromadb.Client()

# Use sentence-transformers for embeddings
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="all-MiniLM-L6-v2"
)

# Create a collection (like a table in a database)
collection = chroma_client.create_collection(
    name="rag_demo",
    embedding_function=sentence_transformer_ef,
    metadata={"description": "RAG fundamentals demo"}
)

# Add our chunks to the database
print("Adding chunks to vector database...")

# Use recursive chunks (our best strategy)
for i, chunk in enumerate(recursive_chunks):
    collection.add(
        documents=[chunk],
        ids=[f"chunk_{i}"],
        metadatas=[{"source": "RAG_Guide", "chunk_index": i}]
    )

print(f" Added {len(recursive_chunks)} chunks to ChromaDB")
print(f" Collection: {collection.name}")
print(f" Count: {collection.count()}")

"""# Step 6: Retrieval - Finding Relevant Context"""

def retrieve_context(query: str, n_results: int = 3):
    """
    Given a query, find the most relevant chunks from our knowledge base.
    """
    results = collection.query(
        query_texts=[query],
        n_results=n_results,
        include=["documents", "distances", "metadatas"]
    )
    return results

# Test some queries
test_queries = [
    "What is chunking and why does it matter?",
    "Which vector databases are available?",
    "How do I evaluate a RAG system?",
]

print("=" * 60)
print("RETRIEVAL DEMO")
print("=" * 60)

for query in test_queries:
    print(f"\n Query: '{query}'")
    print("-" * 50)

    results = retrieve_context(query, n_results=2)

    for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):
        similarity = 1 - distance  # Convert distance to similarity
        print(f"\n    Result {i+1} (similarity: {similarity:.3f}):")
        preview = doc[:200].replace('\n', ' ')
        print(f"   {preview}...")

print("\n" + "=" * 60)
print(" The retriever finds semantically relevant chunks, not just keyword matches!")
print("=" * 60)

"""# Step 7: The Complete RAG Pipeline"""

def rag_pipeline(query: str, n_context_chunks: int = 3) -> dict:
    """
    Complete RAG pipeline:
    1. Retrieve relevant context
    2. Format prompt with context
    3. (In production: send to LLM for generation)
    """

    # Step 1: Retrieve
    results = retrieve_context(query, n_results=n_context_chunks)

    # Step 2: Format context
    context_chunks = results['documents'][0]
    formatted_context = "\n\n---\n\n".join(context_chunks)

    # Step 3: Create augmented prompt
    augmented_prompt = f"""You are a helpful assistant. Answer the question based ONLY on the provided context.
If the context doesn't contain the answer, say "I don't have enough information to answer this."

CONTEXT:
{formatted_context}

QUESTION: {query}

ANSWER:"""

    return {
        "query": query,
        "retrieved_chunks": context_chunks,
        "retrieval_scores": [1 - d for d in results['distances'][0]],
        "augmented_prompt": augmented_prompt,
        "prompt_length": len(augmented_prompt)
    }

# Demo the complete pipeline
demo_query = "What are the best practices for building a RAG system?"

print("=" * 60)
print("COMPLETE RAG PIPELINE DEMO")
print("=" * 60)

result = rag_pipeline(demo_query)

print(f"\n Query: {result['query']}")
print(f"\n Retrieved {len(result['retrieved_chunks'])} chunks:")

for i, (chunk, score) in enumerate(zip(result['retrieved_chunks'], result['retrieval_scores'])):
    print(f"\n   Chunk {i+1} (relevance: {score:.3f}):")
    print(f"   {chunk[:150].replace(chr(10), ' ')}...")

print(f"\n Augmented prompt length: {result['prompt_length']} characters")
print("\n" + "-" * 60)
print(" PROMPT READY FOR LLM:")
print("-" * 60)
print(result['augmented_prompt'][:500] + "...\n[truncated]")

"""# Step 8: Interactive Demo - Try Your Own Queries!"""

def interactive_rag(query: str):
    """Try any query against our RAG system!"""
    if not query.strip():
        return "Please enter a query."

    result = rag_pipeline(query, n_context_chunks=3)

    output = f" Query: {query}\n\n"
    output += " Retrieved Context:\n"
    output += "-" * 40 + "\n"

    for i, (chunk, score) in enumerate(zip(result['retrieved_chunks'], result['retrieval_scores'])):
        output += f"\n[Chunk {i+1}] Relevance: {score:.2%}\n"
        output += chunk[:300] + "...\n"

    output += "\n" + "=" * 40
    output += f"\n Ready to send to LLM ({result['prompt_length']} chars)"
    output += "\n In production, this prompt goes to GPT-4/Claude/Llama for the final answer."

    return output

# Try some queries!
print(interactive_rag("How do I choose the right chunk size?"))
