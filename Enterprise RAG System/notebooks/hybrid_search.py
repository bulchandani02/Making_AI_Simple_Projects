# -*- coding: utf-8 -*-
"""hybrid_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fd0TNVdpcVxcdgt3HVCOR8jmtOd4OhAm

# Step 1: Install Dependencies
"""

!pip install langchain langchain-community langchain-text-splitters chromadb sentence-transformers rank-bm25 -q
!pip install pinecone-client weaviate-client qdrant-client -q  # Vector DB clients
print("All packages installed!")

"""# Step 2: Load TechCorp Dataset"""

# We'll use our consistent TechCorp dataset
# In production, you'd load from files - here we embed it for Colab simplicity

TECHCORP_DOCS = {
    "employee_handbook": """
# TechCorp Employee Handbook
## Chapter 2: Employment Policies

### 2.1 Working Hours
Standard Hours: Monday to Friday, 9:00 AM to 6:00 PM local time.
Flexible Work Policy: Core hours are 10:00 AM to 4:00 PM. Outside core hours, employees may adjust their schedule with manager approval.
Remote Work: Employees may work remotely up to 3 days per week. Fully remote positions require VP approval.

### 2.2 Time Off & Leave
#### Paid Time Off (PTO)
| Tenure | Annual PTO |
| 0-2 years | 15 days |
| 2-5 years | 20 days |
| 5+ years | 25 days |

PTO does not roll over. Maximum carryover: 5 days with manager approval.

#### Sick Leave
Employees receive 10 days of paid sick leave annually. Sick leave does not roll over.
For absences exceeding 3 consecutive days, a doctor's note is required.

#### Parental Leave
- Primary Caregiver: 16 weeks fully paid
- Secondary Caregiver: 6 weeks fully paid
- Must be taken within 12 months of birth/adoption

### 3.1 Pay Structure
Pay Frequency: Semi-monthly (15th and last day of month)
Pay Grades:
| Grade | Title Range | Salary Range (USD) |
| L1 | Junior/Associate | $60,000 - $85,000 |
| L2 | Mid-level | $85,000 - $120,000 |
| L3 | Senior | $120,000 - $160,000 |
| L4 | Staff/Principal | $160,000 - $220,000 |
| L5 | Director | $200,000 - $280,000 |

### 3.3 Health Insurance
TechCorp offers comprehensive health coverage:
Medical Plans:
- PPO Premium: $50/month employee, $200/month family
- PPO Standard: $0/month employee, $100/month family
- HDHP + HSA: $0/month, company contributes $1,500/year to HSA

Coverage begins: First day of employment

### 3.5 Additional Benefits
Learning & Development: $2,000/year for courses, conferences, books
Wellness Stipend: $100/month for gym, fitness apps, mental health
Home Office Stipend: One-time $500 for home office setup
Employee Referral Bonus: $5,000 for engineering roles, $3,000 for others
""",

    "technical_architecture": """
# TechCorp Platform Technical Architecture

## 1. System Overview
TechCorp Platform is a cloud-native, AI-powered enterprise software solution.
Key Statistics:
- 500+ enterprise customers
- 2 million daily active users
- 99.95% uptime SLA
- Processing 50 million API requests/day

## 2. Infrastructure
Primary Cloud: Amazon Web Services (AWS) — US and EU regions
Secondary Cloud: Microsoft Azure — APAC regions

Kubernetes Clusters:
- Production: 3 clusters (us-east, eu-west, ap-southeast)
- Staging: 1 cluster (us-east)

## 3. Core Services
| Service | Language | Database | Owner Team |
| api-gateway | Go | Redis | Platform |
| auth-service | Go | PostgreSQL | Security |
| user-service | Python | PostgreSQL | Platform |
| workflow-engine | Java | PostgreSQL, Redis | Workflows |
| analytics-service | Python | ClickHouse | Data |
| ml-inference | Python | Redis | AI/ML |

### API Gateway
Technology: Kong Gateway (Enterprise)
Rate limiting: 1000 req/min (standard), 10000 req/min (enterprise)
Production endpoint: https://api.techcorp.com

Rate Limits by Plan:
| Plan | Requests/min | Requests/day |
| Free | 100 | 10,000 |
| Starter | 500 | 50,000 |
| Professional | 2,000 | 500,000 |
| Enterprise | 10,000 | Unlimited |

### ML Inference Service
Available Models:
| Model | Purpose | Latency (p99) | Cost/1K requests |
| text-classifier-v2 | Document classification | 50ms | $0.10 |
| sentiment-v3 | Sentiment analysis | 30ms | $0.05 |
| embeddings-v2 | Text embeddings | 20ms | $0.02 |

## 4. Data Architecture
PostgreSQL (Primary): Version 15.4, AWS RDS Multi-AZ
Redis (Caching): Version 7.0, AWS ElastiCache
ClickHouse (Analytics): Self-managed, 6 nodes
Elasticsearch (Search): Version 8.11, AWS OpenSearch

Data Retention:
| Data Type | Retention |
| User data | Account lifetime + 90 days |
| Workflow logs | 90 days |
| Audit logs | 7 years |
| Analytics | 2 years |

## 7. Monitoring
Platform: Datadog
Key Metrics Targets:
- API latency (p99): < 200ms, Alert: > 500ms
- Error rate: < 0.1%, Alert: > 1%
- CPU utilization: < 70%, Alert: > 85%

Incident Severity:
| Severity | Response Time |
| P0 (Complete outage) | 15 min |
| P1 (Major feature broken) | 1 hour |
| P2 (Minor feature broken) | 4 hours |
""",

    "security_policies": """
# TechCorp Security Policies

## 1. Data Classification
| Classification | Description | Examples |
| Public | Freely available | Marketing materials, blog posts |
| Internal | Business information | Org charts, internal memos |
| Confidential | Sensitive business info | Financial reports, roadmaps |
| Restricted | Highly sensitive, regulated | Customer PII, credentials, keys |

## 2. Access Control Policy
### 2.1 Principle of Least Privilege
Users receive minimum access necessary to perform their job functions.

### 2.3 Privileged Access
Requirements:
- Security team approval required
- Background check completed
- Privileged access training completed
- MFA mandatory (hardware key preferred)
- Access reviewed quarterly

Privileged Access Roles:
| Role | Access Level | Approval Required |
| Database Admin | Production databases | Security + VP Eng |
| Infrastructure Admin | AWS/Azure root | Security + CTO |
| Security Admin | IAM, Vault, WAF | CISO |

## 3. Authentication Policy
### Password Requirements
- Length: 12 characters minimum
- Complexity: uppercase, lowercase, number, symbol
- No reuse of last 10 passwords
- Change every 90 days

### Multi-Factor Authentication (MFA)
Required for: All employee accounts, AWS Console, GitHub, VPN, Any system with Confidential data

Approved MFA Methods:
| Method | Use Case | Security Level |
| Hardware key (YubiKey) | Privileged access | Highest |
| Authenticator app | Standard access | High |
| SMS (emergency only) | Account recovery | Medium |

## 7. Incident Response
### Reporting Incidents
Report Immediately To:
- Slack: #security-incidents (24/7 monitored)
- Email: security@techcorp.com
- Phone: +1-555-SEC-RITY (after hours)

Incident Severity:
| Severity | Definition | Response Time |
| Critical | Active breach, data exfiltration | 15 minutes |
| High | Potential breach, active attack | 1 hour |
| Medium | Contained incident, policy violation | 4 hours |
| Low | Minor issue, no data at risk | 24 hours |

## 9. Third-Party Security
All vendors with access to TechCorp data must:
- Complete security questionnaire
- Provide SOC 2 report or equivalent
- Sign DPA (Data Processing Agreement)
- Undergo annual review
""",

    "org_structure": """
# TechCorp Organization Structure
Total Employees: 850

## Executive Leadership Team
| Role | Name | Location |
| CEO | Sarah Chen | San Francisco |
| CTO | Marcus Williams | San Francisco |
| CFO | Jennifer Park | San Francisco |
| COO | David Thompson | New York |
| CISO | Robert Martinez | San Francisco |
| CPO (Product) | Emily Zhang | San Francisco |

## Engineering Division (350 employees)
Head: Marcus Williams (CTO)

### Platform Engineering (80 people)
VP: James Liu
Teams:
- API Platform (Lisa Wang, 15 people): API Gateway, Developer Experience
- Infrastructure (Kevin Patel, 20 people): Kubernetes, AWS, Azure
- Data Platform (Maria Santos, 18 people): Databases, Kafka, Analytics
- SRE (Nina Petrov, 15 people): Reliability, On-call, Monitoring

### AI/ML Team (50 people)
VP: Dr. Andrew Chen
Teams:
- ML Platform (Jessica Zhou, 15 people): Model Training, Inference
- Applied ML (Ryan Murphy, 20 people): Product ML Features
- Research (Dr. Wei Lin, 10 people): R&D, New Capabilities

### Security Engineering (25 people)
VP/CISO: Robert Martinez
Teams:
- AppSec (Tom Wilson, 10 people): Code Review, Pen Testing
- SecOps (Diana Moore, 10 people): SOC, Incident Response

## Office Locations
San Francisco HQ: 400 employees - Engineering, Product, Executive
New York: 200 employees - Sales, Marketing, Customer Success
London: 150 employees - EMEA Operations, Engineering
Singapore: 100 employees - APAC Operations, Support

## Career Levels
Individual Contributors:
| Level | Title | Years Experience |
| L1 | Junior/Associate | 0-2 years |
| L2 | Mid-level | 2-4 years |
| L3 | Senior | 4-7 years |
| L4 | Staff/Principal | 7-10 years |
| L5 | Principal | 10+ years |

Management:
| Level | Title | Scope |
| M1 | Manager | 3-8 direct reports |
| M2 | Senior Manager | Multiple teams |
| M3 | Director | Department |
| M4 | VP | Division |
"""
}

print("TechCorp Dataset Loaded!")
print(f"Documents: {len(TECHCORP_DOCS)}")
for name, content in TECHCORP_DOCS.items():
    print(f" - {name}: {len(content.split())} words")

"""# Step 3: Chunk the Documents"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n## ", "\n### ", "\n\n", "\n", " "]
)

# Chunk all documents with metadata
all_chunks = []
chunk_metadata = []

for doc_name, content in TECHCORP_DOCS.items():
    chunks = splitter.split_text(content)
    for i, chunk in enumerate(chunks):
        all_chunks.append(chunk)
        chunk_metadata.append({
            "source": doc_name,
            "chunk_index": i,
            "doc_type": doc_name.replace("_", " ").title()
        })

print(f"Created {len(all_chunks)} chunks from {len(TECHCORP_DOCS)} documents")

# Preview a chunk
print(f"\nSample chunk from '{chunk_metadata[5]['source']}':")
print("-" * 50)
print(all_chunks[5][:300] + "...")

"""# Step 4: The Problem with Vector Search Alone"""

"""
## Why Vector Search Isn't Enough

Vector search is GREAT at finding semantically similar content.
But it has blind spots:

1. **Exact matches**: "Error code 5012" - vectors might miss exact codes
2. **Proper nouns**: "Kevin Patel" - names get fuzzy in embedding space
3. **Acronyms**: "SLA" vs "Service Level Agreement"
4. **Numbers**: "99.95% uptime" - vectors struggle with specific numbers

## The Solution: Hybrid Search

Combine the best of both worlds:
- **BM25 (Keyword)**: Great for exact matches, rare terms, proper nouns
- **Vector (Semantic)**: Great for meaning, synonyms, concepts

Then **re-rank** with a cross-encoder for precision.
"""

print("Vector Search Blind Spots:")
print("   Exact codes: 'error 5012' → might miss exact match")
print("   Names: 'Kevin Patel' → fuzzy in embedding space")
print("   Numbers: '99.95%' → vectors struggle with specifics")
print("   Acronyms: 'SLA' ≠ 'Service Level Agreement' in vectors")
print("\n   Solution: Hybrid Search (BM25 + Vectors) + Re-ranking")

"""# Step 5: BM25 - The Classic That Still Works"""

from rank_bm25 import BM25Okapi
import numpy as np

# Tokenize chunks for BM25
tokenized_chunks = [chunk.lower().split() for chunk in all_chunks]

# Create BM25 index
bm25 = BM25Okapi(tokenized_chunks)

def bm25_search(query: str, top_k: int = 5):
    """Search using BM25 (keyword-based)"""
    tokenized_query = query.lower().split()
    scores = bm25.get_scores(tokenized_query)

    # Get top-k indices
    top_indices = np.argsort(scores)[::-1][:top_k]

    results = []
    for idx in top_indices:
        results.append({
            "chunk": all_chunks[idx],
            "score": scores[idx],
            "metadata": chunk_metadata[idx]
        })
    return results

# Test BM25 with exact term search
print("=" * 60)
print("BM25 SEARCH TEST")
print("=" * 60)

test_query = "Kevin Patel Infrastructure"
results = bm25_search(test_query, top_k=3)

print(f"\n Query: '{test_query}'")
print("-" * 50)
for i, r in enumerate(results):
    print(f"\n[{i+1}] Score: {r['score']:.2f} | Source: {r['metadata']['source']}")
    print(f"    {r['chunk'][:150]}...")

"""# Step 6: Vector Search (Semantic)"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load embedding model
print("Loading embedding model...")
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

# Embed all chunks
print("Embedding chunks...")
chunk_embeddings = embed_model.encode(all_chunks, show_progress_bar=True)
print(f"Embedded {len(chunk_embeddings)} chunks")

def vector_search(query: str, top_k: int = 5):
    """Search using vector similarity"""
    query_embedding = embed_model.encode([query])
    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]

    top_indices = np.argsort(similarities)[::-1][:top_k]

    results = []
    for idx in top_indices:
        results.append({
            "chunk": all_chunks[idx],
            "score": similarities[idx],
            "metadata": chunk_metadata[idx]
        })
    return results

# Test vector search
print("\n" + "=" * 60)
print("VECTOR SEARCH TEST")
print("=" * 60)

test_query = "who manages the cloud infrastructure team"
results = vector_search(test_query, top_k=3)

print(f"\n Query: '{test_query}'")
print("-" * 50)
for i, r in enumerate(results):
    print(f"\n[{i+1}] Score: {r['score']:.3f} | Source: {r['metadata']['source']}")
    print(f"    {r['chunk'][:150]}...")

"""# Step 7: Hybrid Search - Best of Both Worlds"""

def hybrid_search(query: str, top_k: int = 5, bm25_weight: float = 0.3):
    """
    Combine BM25 and vector search with weighted scoring.

    Args:
        query: Search query
        top_k: Number of results to return
        bm25_weight: Weight for BM25 (0-1). Vector weight = 1 - bm25_weight
    """
    # Get more candidates from each method
    bm25_results = bm25_search(query, top_k=top_k * 2)
    vector_results = vector_search(query, top_k=top_k * 2)

    # Normalize scores to 0-1 range
    bm25_scores = [r['score'] for r in bm25_results]
    vector_scores = [r['score'] for r in vector_results]

    bm25_max = max(bm25_scores) if bm25_scores else 1
    vector_max = max(vector_scores) if vector_scores else 1

    # Combine results with weighted scoring
    combined = {}

    for r in bm25_results:
        chunk_id = r['chunk'][:100]  # Use first 100 chars as ID
        normalized_score = (r['score'] / bm25_max) * bm25_weight
        combined[chunk_id] = {
            "chunk": r['chunk'],
            "bm25_score": r['score'] / bm25_max,
            "vector_score": 0,
            "combined_score": normalized_score,
            "metadata": r['metadata']
        }

    for r in vector_results:
        chunk_id = r['chunk'][:100]
        normalized_score = r['score'] * (1 - bm25_weight)

        if chunk_id in combined:
            combined[chunk_id]['vector_score'] = r['score']
            combined[chunk_id]['combined_score'] += normalized_score
        else:
            combined[chunk_id] = {
                "chunk": r['chunk'],
                "bm25_score": 0,
                "vector_score": r['score'],
                "combined_score": normalized_score,
                "metadata": r['metadata']
            }

    # Sort by combined score
    sorted_results = sorted(combined.values(), key=lambda x: x['combined_score'], reverse=True)
    return sorted_results[:top_k]

# Test hybrid search
print("=" * 60)
print("HYBRID SEARCH TEST")
print("=" * 60)

test_query = "Kevin Patel infrastructure team responsibilities"
results = hybrid_search(test_query, top_k=3, bm25_weight=0.4)

print(f"\n Query: '{test_query}'")
print("-" * 50)
for i, r in enumerate(results):
    print(f"\n[{i+1}] Combined: {r['combined_score']:.3f}")
    print(f"    BM25: {r['bm25_score']:.3f} | Vector: {r['vector_score']:.3f}")
    print(f"    Source: {r['metadata']['source']}")
    print(f"    {r['chunk'][:150]}...")

"""# Step 8: Cross-Encoder Re-ranking (The Secret Sauce)"""

"""
## Why Re-ranking?

Bi-encoders (what we used for vector search) are FAST but approximate.
Cross-encoders are SLOW but much more accurate.

Strategy:
1. Use fast methods (BM25 + vectors) to get top 20 candidates
2. Re-rank those 20 with a cross-encoder
3. Return top 5 with much higher precision

This is what production RAG systems actually use.
"""

from sentence_transformers import CrossEncoder

# Load cross-encoder for re-ranking
print("Loading cross-encoder model...")
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
print("✅ Re-ranker loaded!")

def hybrid_search_with_reranking(query: str, top_k: int = 5, candidates: int = 20):
    """
    Hybrid search with cross-encoder re-ranking.

    1. Get candidates from hybrid search
    2. Re-rank with cross-encoder
    3. Return top-k
    """
    # Step 1: Get candidates from hybrid search
    hybrid_results = hybrid_search(query, top_k=candidates)

    # Step 2: Re-rank with cross-encoder
    pairs = [[query, r['chunk']] for r in hybrid_results]
    rerank_scores = reranker.predict(pairs)

    # Add rerank scores
    for i, r in enumerate(hybrid_results):
        r['rerank_score'] = float(rerank_scores[i])

    # Step 3: Sort by rerank score
    reranked = sorted(hybrid_results, key=lambda x: x['rerank_score'], reverse=True)

    return reranked[:top_k]

# Test with re-ranking
print("\n" + "=" * 60)
print("HYBRID SEARCH + RE-RANKING TEST")
print("=" * 60)

test_query = "What is the process to get production database access?"
results = hybrid_search_with_reranking(test_query, top_k=3)

print(f"\n Query: '{test_query}'")
print("-" * 50)
for i, r in enumerate(results):
    print(f"\n[{i+1}] Rerank Score: {r['rerank_score']:.3f}")
    print(f"    (BM25: {r['bm25_score']:.3f} | Vector: {r['vector_score']:.3f})")
    print(f"    Source: {r['metadata']['source']}")
    print(f"    {r['chunk'][:200]}...")

"""# Step 9: Compare All Methods Side-by-Side

"""

def compare_methods(query: str):
    """Compare BM25, Vector, Hybrid, and Hybrid+Rerank"""

    print("=" * 70)
    print(f"QUERY: '{query}'")
    print("=" * 70)

    methods = {
        "BM25 (Keyword)": bm25_search(query, top_k=1)[0],
        "Vector (Semantic)": vector_search(query, top_k=1)[0],
        "Hybrid (BM25+Vector)": hybrid_search(query, top_k=1)[0],
        "Hybrid + Rerank": hybrid_search_with_reranking(query, top_k=1)[0]
    }

    for method_name, result in methods.items():
        print(f"\n {method_name}")
        print(f"   Source: {result['metadata']['source']}")
        preview = result['chunk'][:120].replace('\n', ' ')
        print(f"   Preview: {preview}...")

    print("\n" + "-" * 70)

# Test queries that highlight differences
test_queries = [
    "99.95% uptime SLA",                           # Exact number - BM25 should win
    "how to report security problems",             # Semantic - Vector should win
    "Kevin Patel team size",                       # Mixed - Hybrid should win
    "What approvals needed for AWS root access",   # Complex - Rerank should win
]

for query in test_queries:
    compare_methods(query)

"""# Step 10: Vector Database Comparison"""

print("Vector Database Decision Guide:")
print()
print("Choose ChromaDB if: Local dev, learning, <100K vectors")
print("Choose Pinecone if: Production SaaS, don't want to manage infra")
print("Choose Weaviate if: Need GraphQL, built-in hybrid search")
print("Choose Qdrant if: High performance, complex filtering")
print("Choose pgvector if: Already on Postgres, budget-conscious")
print("Choose Azure AI Search if: Enterprise, Azure ecosystem")

"""# Step 11: Production-Ready RAG Class"""

class ProductionRAG:
    """
    Production-ready RAG with hybrid search and re-ranking.

    This is what you'd actually deploy.
    """

    def __init__(self, chunks: list, metadata: list, bm25_weight: float = 0.3):
        self.chunks = chunks
        self.metadata = metadata
        self.bm25_weight = bm25_weight

        # Initialize BM25
        tokenized = [c.lower().split() for c in chunks]
        self.bm25 = BM25Okapi(tokenized)

        # Initialize embeddings
        self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.embeddings = self.embed_model.encode(chunks, show_progress_bar=True)

        # Initialize reranker
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        print(f"ProductionRAG initialized with {len(chunks)} chunks")

    def search(self, query: str, top_k: int = 5, use_reranking: bool = True) -> list:
        """
        Search with hybrid + optional reranking.
        """
        # BM25 search
        tokenized_query = query.lower().split()
        bm25_scores = self.bm25.get_scores(tokenized_query)

        # Vector search
        query_embedding = self.embed_model.encode([query])
        vector_scores = cosine_similarity(query_embedding, self.embeddings)[0]

        # Normalize and combine
        bm25_norm = bm25_scores / (max(bm25_scores) + 1e-6)

        combined_scores = (
            self.bm25_weight * bm25_norm +
            (1 - self.bm25_weight) * vector_scores
        )

        # Get top candidates
        candidates_k = top_k * 4 if use_reranking else top_k
        top_indices = np.argsort(combined_scores)[::-1][:candidates_k]

        results = []
        for idx in top_indices:
            results.append({
                "chunk": self.chunks[idx],
                "metadata": self.metadata[idx],
                "bm25_score": float(bm25_norm[idx]),
                "vector_score": float(vector_scores[idx]),
                "combined_score": float(combined_scores[idx])
            })

        # Rerank if requested
        if use_reranking and len(results) > 0:
            pairs = [[query, r['chunk']] for r in results]
            rerank_scores = self.reranker.predict(pairs)

            for i, r in enumerate(results):
                r['rerank_score'] = float(rerank_scores[i])

            results = sorted(results, key=lambda x: x['rerank_score'], reverse=True)

        return results[:top_k]

    def query(self, question: str, top_k: int = 3) -> dict:
        """
        Full RAG query - search + format for LLM.
        """
        results = self.search(question, top_k=top_k, use_reranking=True)

        context = "\n\n---\n\n".join([r['chunk'] for r in results])
        sources = [r['metadata']['source'] for r in results]

        prompt = f"""Answer the question based on the following context from TechCorp documentation.
If the answer is not in the context, say "I don't have information about that."

CONTEXT:
{context}

QUESTION: {question}

ANSWER:"""

        return {
            "prompt": prompt,
            "context_chunks": results,
            "sources": list(set(sources))
        }

# Initialize production RAG
print("Initializing Production RAG system...")
rag = ProductionRAG(all_chunks, chunk_metadata, bm25_weight=0.35)

"""# Step 12: Test the Production RAG"""

test_questions = [
    "How many days of PTO do I get after 3 years?",
    "Who is Kevin Patel and what does he manage?",
    "What's the process for reporting a security incident?",
    "What is the API rate limit for enterprise customers?",
    "How do I get access to production databases?",
]

print("=" * 70)
print("PRODUCTION RAG TESTS")
print("=" * 70)

for question in test_questions:
    print(f"\n {question}")
    print("-" * 60)

    result = rag.query(question, top_k=2)

    print(f"Sources: {', '.join(result['sources'])}")
    print(f"Top result rerank score: {result['context_chunks'][0]['rerank_score']:.3f}")
    print(f"Context preview:")
    print(f"   {result['context_chunks'][0]['chunk'][:200]}...")
